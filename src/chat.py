import logging
import sys
import openai
import os

from llama_index import GPTTreeIndex, GPTListIndex, SimpleDirectoryReader, LLMPredictor, PromptHelper, ServiceContext
from IPython.display import Markdown, display
from langchain import OpenAI

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))


# My OpenAI Key

openai.api_key = ""

documents = SimpleDirectoryReader('prompt').load_data()

llm_predictor = LLMPredictor(llm=OpenAI(model_name="text-davinci-003",
                                       temperature=1.0,
                                       top_p=1.0,
                                       frequency_penalty=0.5,
                                       presence_penalty=1.0,
                                       max_tokens=1024))

service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)

"""

1. `temperature`: This parameter controls the randomness of the generated text. A higher value (e.g., 1.0) will make the output more diverse and creative, while a lower value (e.g., 0.1) will make the output more deterministic and focused on the most likely completion. A temperature of 0 will result in the most deterministic output, but it may be repetitive.

2. `top_p`: This parameter is related to the nucleus sampling technique. It controls the fraction of the total probability mass for the token sampling. A value of 1.0 means that all tokens will be considered, while a smaller value (e.g., 0.9) will only consider the top tokens that cover 90% of the cumulative probability. Lower values of `top_p` can lead to more focused and coherent text but may be less diverse.

3. `frequency_penalty`: This parameter allows you to control the penalty applied to tokens based on their frequency in the model's training data. A positive value (e.g., 1.0) will encourage the model to generate less frequent words, whereas a negative value (e.g., -1.0) will encourage the model to generate more frequent words. A value of 0.0 means no penalty will be applied based on token frequency.

4. `presence_penalty`: This parameter allows you to control the penalty applied to tokens that have already been generated in the text. A positive value (e.g., 1.0) will discourage the model from repeating words, while a negative value (e.g., -1.0) will encourage repetition. A value of 0.0 means no penalty will be applied based on token presence.

5. `max_tokens`: This parameter sets the maximum number of tokens (words or word pieces) to be generated by the model. It's essential to set an appropriate value to prevent the generation of overly long or short text. Note that if the specified value is too low, the generated text might be cut-off and not make sense.

These parameters can be adjusted to control the trade-off between creativity, diversity, coherence, and focus in the generated text. The optimal values may vary depending on the specific use case and desired output.
"""
davinci_index = GPTListIndex.from_documents(documents, service_context=service_context)

response = davinci_index.query("Please plan a trip this summer that cites specific events and attractions including pricing and where to get tickets. Stay focused on a single location. Always address me by first name and respond in an incredibly friendly manner.")
#print(display(Markdown(f"<b>{response}</b>")))

print(response)
